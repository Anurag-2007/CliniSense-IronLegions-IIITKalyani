# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12U8udlOiaNgUU22uFpvBfc7EHqkvJj2T
"""

import uuid
from typing import List, Dict
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import Document
import re
import json
import os

EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
LLM_MODEL = "gpt-4o-mini"  # or your fine-tuned endpoint
TOP_K = 5
CHUNK_SIZE = 512
CHUNK_OVERLAP = 128

"""####LOADING THE CLINICAL DATA AND CONVERTING IT INTO CHUNKS RECURSIVELY(USING VARIOUS DELIMITERS)"""

def load_clinical_data(data: str) -> List[Document]:
  splitter = RecursiveCharacterTextSplitter(chunk_size = CHUNK_SIZE, chunk_overlap = CHUNK_OVERLAP)
  chunks = splitter.split_text(data)
  return [
      Document(
          page_content=chunk, metadata={"chunk_id":str(uuid.uuid4())}
      ) for chunk in chunks
  ]

"""####Embed the chunks and store in faiss(use later for searching similar words)"""

embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

def embed_chunks(chunks: List[Document]) -> FAISS:
  chunks_list = [c.page_content for c in chunks]
  metadatas_list = [m.metadata for m in chunks]

  return FAISS.from_texts(chunks_list, embedding_model, metadatas=metadatas_list)

"""

> Later add hugging face embedded model authentication(anurag will handle this)

####Using GPT-o4 feed the chunks into the model and get a reportive diagnosis
"""

def retrieve_relevant_chunks(query: str, vectorstore: FAISS) -> List[Dict]:
    docs = vectorstore.similarity_search_with_score(query, k=TOP_K)
    results = []
    for doc, score in docs:
        chunk_id = doc.metadata["chunk_id"]
        # Mark citation
        marked = f"<ref>{chunk_id[:8]}</ref> {doc.page_content}"
        results.append({"text": marked, "chunk_id": chunk_id, "score": score})
    return results


PROMPT = PromptTemplate.from_template(
"""
You are an expert clinical reasoning assistant.
Given the retrieved clinical note excerpts below, produce:

1. A concise factual summary (â‰¤ 50 words) of the patient's current condition.
2. A prioritized differential diagnosis list (max 6 items) with:
   - Diagnosis name
   - Confidence level (High/Medium/Low)
   - Brief justification (1 sentence)
   - Exact citation(s) from the retrieved text (use <ref>xxxxx</ref> format)

--- Retrieved Evidence ---
{evidence}
--- End Evidence ---

Respond **only** in valid JSON matching this schema:
{{
  "summary": "...",
  "differential_diagnoses": [
    {{
      "diagnosis": "...",
      "confidence": "High|Medium|Low",
      "justification": "...",
      "citations": ["<ref>1</ref>", "<ref>3</ref>"]
    }}
  ]
}}
"""
)

def parse(truncated_json):
    # Step 1: Extract everything inside the outermost { }
    json_match = re.search(r"\{.*", truncated_json, re.DOTALL)
    if not json_match:
        raise ValueError("No JSON object found")

    json_str = json_match.group(0)

    # Step 2: Close unclosed arrays and objects
    open_braces = json_str.count('{') - json_str.count('}')
    open_brackets = json_str.count('[') - json_str.count(']')

    # Close missing braces/brackets
    json_str += '}' * open_braces
    json_str += ']' * open_brackets

    # Step 3: Fix incomplete last object (e.g., "diagn)
    # Find last incomplete key-value
    json_str = re.sub(r',\s*$', '', json_str)  # remove trailing comma
    json_str = re.sub(r'"\s*$', '": null', json_str)  # fix cut-off key

    # Step 4: Parse
    try:
        data = json.loads(json_str)
        return data
    except json.JSONDecodeError as e:
        print(f"JSON still invalid after fix: {e}")
        raise


key = os.environ.get("KEY")
llm = OpenAI(model=LLM_MODEL, temperature=0.0, openai_api_key=key)
def generate_summary(note_text: str) -> Dict:
# 1. Chunk & index
    docs = load_clinical_data(note_text)
    vectorstore = embed_chunks(docs)

    # 2. Use first chunk as query (or summarize whole note)
    query = note_text[:1000]
    retrieved = retrieve_relevant_chunks(query, vectorstore)

    evidence = "\n\n".join([f"[{i+1}] {r['text']}" for i, r in enumerate(retrieved)])

    # 3. Prompt
    prompt = PROMPT.format(evidence=evidence)

    # 4. LLM
    raw = llm(prompt)

    try:
        parsed = parse(raw)
        
        # Extract what you want
        summary = parsed.get("summary", "")
        differential_diagnoses = parsed.get("differential_diagnoses", [])

        # Output as clean JSON
        result = {
            "summary": summary,
            "differential_diagnoses": differential_diagnoses
        }

        return result

    except Exception as e:
        print("Failed:", e)
        return e